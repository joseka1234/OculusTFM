\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{hyperref}
\author{José Carlos Rodríguez Cortés}
\title{Cuaderno Bitácora TFM}
\begin{document}
\maketitle
\begin{abstract}
Cuaderno en el que se apuntarán los eventos sucedidos durante cada día de investigación para el TFM para su posterior estudio y/o integración a la memoria final del trabajo, así como al artículo.
\end{abstract}
\break

\section{Día 1: 20/06/2017}

\subsection{Hechos}

Toma de contacto con Oculus SDK y Unity.
Implementación de los algoritmos preparados con anterioridad. A saber:

\begin{itemize}
\item Algoritmo de movimiento mediante raycast. Implementando para ello 3 tipos de movimientos:
	\begin{itemize}
	\item Teletransporte
	\item Desplazamiento bidimensional sobre un plano con tiempo constante, es decir, sea cual sea la distancia a recorrer siempre se tardará lo mismo, por lo que algunos movimiento serán más acelerados que otros.
	\item Desplazamiento tridimensional. Se ejecuta un desplazamiento bidimensional con un "salto" implementado a partir de una función senoidal con una algura máxima. También tiene una duración constante por lo que puede ser un movimiento más veloz si apuntamos a zonas distantes del mapa.
	\end{itemize}
\item Algoritmo de foco dinámico sin necesidad de eyetracking utilizando para ello el artículo \cite{Porcino et al.}.
\end{itemize}

\subsection{Observaciones}

Tras hacer algunas pruebas de compatibilidad de estos algoritmos con las gafas Oculus Rift se prevee buen funcionamiento de todos ellos para escenarios más complejos. A falta de testear estos algoritmos con más personas se asumirá que funcionan y no causan fatiga.

Se ha detectado que en el enfoque dinámico el paso de enfocado a desenfocado y viceversa es demasiado brusco.

\subsection{Trabajo Futuro}

Para el próximo día se plantean las siguientes tareas:
\begin{itemize}
\item Implementar interacción de agarrar y soltar. Idea: Utilizar OVRGrabber y OVRGrabbable.
\item Intentar implementar manos virtuales a partir de modelos gratuitos de internet para mejorar la experiencia dentro del mundo virtual.
\end{itemize}

\section{Día 2: 22/06/2017}

\subsection{Hechos}

Ajustados algunos parámetros de los algoritmos probados el Día 1. Sin cambios observables en la visualización final.

Se intenta implementar funcionalidad para agarrar objetos mediante el OVRGrabbable y OVRGrabber de la librería OVR de Oculus SDK, sin embargo, durante la búsqueda de información para esta implementación se encuentra un SDK que abstrae el oficial de Oculus llamado NewtonVR.

Se despliega una escena sencilla con este nuevo SDK y se hacen pruebas de interacción.
Se integra Avatar SDK para potenciar la visualización de la escena y la inmersión del jugador.

Tras hacer algunas pruebas con el Avatar SDK en escenas independientes se integra Avatar SDK a la escena realizada con NewtonVR resultando en una escena en la que podemos ver nuestras manos virtuales además de interactuar con algunos objetos a los que se les aplican físicas.

\subsection{Observaciones}

Tras utilizar NewtonVR para implementar una escena sencilla se observa que es mucho más sencillo crear escenas con este nuevo SDK que con el SDK básico de Oculus por lo que se plantean nuevos desarrollos a partir del mismo.

Avatar SDK es extremadamente sencillo de incorporar a cualquier escena por lo que será en extremo útil para crear una mejor experiencia.

\subsection{Trabajo Futuro}

Para las próximas sesiones se plantea buscar nuevas formas de interacción e implementarlas, intentar implementar Go-Go en alguna o todas sus variantes, además de buscar algunas escenas complejas en internet para integrarlas al proyecto y comenzar a crear inmersión.\\
\textbf{Idea}: Crear una escena de una bolera. Así se podrían aprovechar las interacciones que se han implementado en esta sesión con lanzamiento de bolas y derribar objetos verticales.\\
\textbf{Idea}: Basar el trabajo en un pequeño minijuego deportivo inmersivo (bolos, baloncesto, etc...).

\section{Día 3: 24/06/2017}

\subsection{Hechos}

Se cambia la interacción del usuario con el entorno al forzarle a utilizar un gesto para desplazarse, a saber, apuntar con el dedo índice.
Esto se ve reforzado con el Avatar SDK ya que se pueden ver exactamente los movimientos que están haciendo las manos.

Se integran "Mallas de Navegación" (Nav Meshes) así como agentes de navegación a una escena sencilla para que la navegación se realice utilizando algoritmos de \textit{pathfinding}.

Se comienza a experimentar con escenas complejas.

\subsection{Observaciones}

Al cambiar la interacción se le da al usuario mayor juego a la hora de interactuar con la escena ya que, al no tener siempre la esfera que muestra el destino, se consigue una mayor inmersión.

Utilizando las mallas de navegación conseguimos un movimiento mucho más natural por parte del usuario, evitando así \textit{"atravesar"} objetos del escenario, lo cual, sacaría al usuario de la experiencia inmersiva.

Integrar escenas complejas supone un nuevo problema de navegación ya que hay que adaptar las mallas de navegación a cada parte de la escena.

También se han detectado problemas dependiendo del modelo que se utiliza por lo que se asume que el problema lo tiene el modelo en sí y no la implementación.


\subsection{Trabajo Futuro}

Adaptar algunas escenas complejas a la navegación con mallas de navegación para que resulte más inmersivo para el usuario.

Cambiar la estética del objeto de destino, en vez de utilizar una bola, utilizar un emisor de partículas.

Comenzar a integrar objetos interactuables a las escenas complejas.

\section{Día 4: 27/06/2017}

\subsection{Hechos}

Se ha adaptado el script de movimiento para funcionar mejor con escenas complejas.
Creada escena compleja utilizando Avatar SDK y NewtonVR para representar al usuario, dando como resultado un entorno inmersivo y complejo.

Se ha adaptado el script de movimiento para adaptar la dirección en la que mira el usuario cuando se mueve para reducir la sensación de desorientación.

Se ha probado a cambiar la esfera de destino por un emisor de partículas.

Creado un panel para informar al usuario que tipo de movimiento está empleando.

\subsection{Observaciones}

Integrando lo ya visto antes de NewtonVR y el Avatar SDK a una escena compleja se obtiene como resultado una escena sorprendentemente inmersiva por si misma.

La adaptación de la dirección para disminuir la desorientación al desplazarse puede llegar a causar mareo. Habría que dar la opción de quitarlo o hacer una encuesta en la que se pregunté sobre que tipo de desplazamiento se prefiere.

El cambio de la esfera al emisor de partículas ha dado buen resultado visual. No obstante habría que reforzar más este indicador ya que: el emisor de partículas no está adecuadamente configurado y hace falta algo más para dar bien el feedback al usuario de en dónde va a aparecer o hacia donde va a desplazarse. En lo referente al desplazamiento se prefiere precisión antes que inmersión.

El panel, aunque supone un elemento extradiegético que saca al usuario un poco de la experiencia, se considera necesario para que el usuario no se "pierda" en cuanto al tipo de movimiento que va a utilizar.\\
\textbf{Idea}: Usar algún elemento intradiegético para mostrar esta información, tal como un reloj en el brazo, alguna interfaz futurista o incluso cambiar el elemento visual que nos dice el tipo de movimiento por un elemento auditivo.

\subsection{Trabajo Futuro}

Trabajar en la integración de sonidos a la escena para aumentar la inmersión.
Cambiar algunos elementos informativos de extradiegéticos a intradiegéticos para que la experiencia sea más inmersiva.

Implementar la navegación mediante pathfinding en escenarios complejos.
Investigar técnicas para reducir el mareo aparte del enfoque dinámico que se puedan aplicar a este proyecto.

Comenzar a implementar scripts similares en Unreal Engine para ir teniendo algo sobre lo que trabajar en este motor.

\section{Día 5: 28/06/2017}

\subsection{Hechos}

Cambiado el script de control con raycast y divididos los movimientos en distintas clases aplicando así el patrón de diseño \textbf{Estrategia}.

\subsection{Observaciones}

Al implementar el movimiento a partir del patrón estrategia se aumenta la capacidad de modificación del script de control y se mejora la claridad del código.

\subsection{Trabajo Futuro}

Sin cambios en cuanto al trabajo futuro de los días anteriores.

\section{Día 6: 29/06/2017}

\subsection{Hechos}

Tras estudiar el artículo \cite{Davis et al.} y viendo las conclusiones del mismo se aprecia que surgen menos mareos en escenas menos complejas.

Debido a esto se incorpora un nuevo modelo para hacer pruebas. Este nuevo modelo está formado a partir de texturas mucho más sencillas que el anterior modelo por lo que se espera que el uso del mismo reduzca notablemente el posible mareo que puedan tener los futuros usuarios.

Se ha implementado un sistema de profundidad de campo dinámica similar a la mostrada en el artículo \cite{Carnegie and Rhee}.

Añadidos sonidos de ambiente para las escenas.

\subsection{Observaciones}

El nuevo modelo más simple supone una carga computacional y de renderizado bastante menor que los otros modelos más complejos por lo que, aparte de suponer un factor de reducción de la fatiga a partir del uso de modelos menos complejos, se verá incrementada la velocidad de la escena por lo que el usuario sentirá aún menos fatiga.

El nuevo sistema de enfoque dinámico implementado a partir de la profundidad de campo dinámica se comporta de una manera mucho mejor que el anterior sistema. Con un desenfoque de mayor calidad y un enfoque mucho más efectivo y suave que antes.

Se espera que con algún sonido de ambiente para las escenas complejas se añada una capa más de inmersión.

\subsection{Trabajo Futuro}

Ajustar el sistema de enfoque dinámico para que los valores de blur no sean demasiado molestos para el usuario.

Probar la nueva escena menos compleja y adaptar las mallas de navegación para la escena con "jugador" usando VR HMD.

Añadir los sonidos a la escena y buscar música suave (de ambiente) para crear una distracción dentro del entorno virtual y que el usuario se sienta más inmerso en dicho mundo.

Mirar video \url{https://www.youtube.com/watch?v=p0YxzgQG2-E} para ejemplos de movimiento.

\section{Día 7: 30/06/2017}

\subsection{Hechos}

Se han estudiado algunos artículos para añadir distintos tipos de interacciones y movimientos a la escena.
Se ha comenzado a implementar el movimiento de correr en base a muestras de aceleración recogidas del propio usuario caminando y/o corriendo.

\subsection{Observaciones}

Algunas de las interacciones y técnicas de movimiento han resultado muy interesantes para implementar en nuestra escena. Para el proyecto que se está realizando ha destacado sobre otras el sistema de movimiento basado en correr en el sitio ya que hace que el usuario se sienta más metido en el entorno.
Existen otros sistemas de movimiento que se han descartado por \textit{"romper"} con la pretensión de inmersión que se desea conseguir.

\subsection{Trabajo Futuro}

Implementar sistemas de movimiento basados en lanzamiento de proyectil.
Aunque se ha asumido el consenso de que la técnica de interacción de \textit{"mano virtual"} es la que mejor se adapta para que el usuario se sienta inmerso en el entorno presentado, se planea estudiar e implementar nuevos sistemas de interacción con objetos para dar más variedad de posibilidades al usuario.

\section{EXTRA}

En el tiempo transcurrido desde el día 7 al día 8 se han hecho pruebas con el Oculus Rift DK1 para adaptarlo a mi ordenador personal con resultados negativos ya que no se ha podido adaptar el aparato al proyecto y únicamente se ha podido ejecutar la demostración (la demostración del escritorio) que viene incluida en la instalación.
Además de esto se han estudiado los artículos planteados por el profesor Pedro Cano en \cite{Google Docs} así como el libro \textit{"The VR Book"}  \cite{Jerald 2016}.


\section{Día 8: 06/07/2017}

\subsection{Hechos}

Tras estudiar los patrones de interacción propuestos en \cite{Jerald 2016} y más concretamente el patrón de caminar (\textit{Walking Pattern}) \cite[28.3.1, pp. 336-338]{Jerald 2016} se ha adaptado el script de movimiento correspondiente al movimiento caminando para que funcione correctamente.

\subsection{Observaciones}

Se ha considerado que este tipo de movimiento podría resultar bastante beneficioso a la inmersión ya que el usuario tiene que realizar la acción de forma física para poder moverse. Además, este sistema de interacción puede favorecer a reducir el ciber-mareo.

\subsection{Trabajo Futuro}

Probar esta modificación en el laboratorio para comprobar que funcione correctamente.
Aplicar algunas de las técnicas que aparecen en \cite{Jerald 2016}.

\section{Día 9: 07/07/2017}

\section{Hechos}

Se ha adaptado el modelo \textit{"sponza"} pasado por el profesor al modelo de navegación propuesto.

Se ha adaptado el movimiento de teletransporte para adaptarse a lo visto en \cite{Jerald 2016} con un \textit{"fade-in"} y un \textit{"fade-out"} para reducir la desorientación y el mareo.

\section{Observaciones}

El modelo \textit{"sponza"} es fácilmente adaptable y ofrece un aspecto que puede tener mucho potencial para crear una escena inmersiva en VR. Se ha tenido que adaptar la cámara para este modelo, poniendo el plano lejano más alejado que en los otros modelos ya que utiliza unas escalas mucho más grandes.

Al adaptar el teletransporte a lo visto en \cite{Jerald 2016} se consigue un movimiento de teletransporte más suave y que, probablemente, produzca menos mareo.

\section{Trabajo Futuro}

Probar el nuevo modelo en el laboratorio y adaptar mejor la navegación por él. Modelar la iluminación de este modelo y diseñar algún tipo de interacción para poder moverse entre las distintas plantas del edificio modelado para añadir cierta navegación vertical.

Probar el nuevo movimiento de teletransporte para comprobar si realmente se reduce la sensación de desorientación y mareo al añadir el \textit{"fade-in"} y \textit{"fade-out"}.

\section{Día 10: 08/07/2017}

\subsection{Hechos}

De acuerdo con lo visto en \cite{Unity Tutorials} se ha adaptado la interfaz que muestra el movimiento actual para adaptarse a lo visto en dicho tutorial.

De acuerdo a lo estudiado en \cite{Vanhatalo} se pretende implementar un sistema de reconocimiento de primitivas geométricas básico para crear nuevas interacciones gestuales para el usuario final.

Se pretende acuñar una metáfora de interacción basado en \textbf{"Varita Mágica"} en la que, al realizar una serie de movimiento o figuras en el aire con una varita (que será un elemento intradiegético) realizaremos una serie de acciones diferentes.

Esta idea se extrae de algunos artículos estudiados tales como \cite{Rausch et al.} o \cite{Clifford et al.}.

\subsection{Observaciones}

Al utilizar este sistema para las interfaces de usuario se consigue una mayor adaptabilidad de las mismas así como una mayor facilidad para crearlas y modificarlas.

Se está comenzando a implementar el sistema de detección de patrones de dibujado. A pesar de ser un algoritmo bastante sencillo se detectan ciertos problemas de optimización.
Existe ciertos trabajos que optimizan lo visto en \cite{Vanhatalo}.

\subsection{Trabajo Futuro}

Probar el nuevo sistema de interfaz de usuario con las gafas de RV en el laboratorio.

Implementar algoritmos optimizados para el detector de patrones de dibujo usado para la metáfora de interacción de \textbf{"Varita Mágica"}.

\section{Día 11: 09/07/2017}

\subsection{Hechos}

Se ha terminado con la implementación del sistema que detecta patrones de dibujado para la metáfora de interacción de \textbf{"Varita Mágica"}.

\subsection{Observaciones}

A falta de realizar pruebas experimentales, el algoritmo parece que dará buenos resultados, no obstante, el algoritmo se aprecia como bastante ineficiente ya que realiza múltiples cálculos para detectar los patrones.

Un punto a favor que tiene este algoritmo es que no requiere de entrenamiento previo ya que el se basa en cálculos matemáticos sobre geometría y el artículo de dónde lo he extraído \cite{Vanhatalo} ya proporciona parámetros para reconocer varias figuras.

Por lo pronto se están reconociendo hasta cuatro figuras diferentes para implementar 4 acciones diferentes que se definirán en un futuro. Estas figuras geométricas son:

\begin{itemize}
\item Linea
\item Circulo
\item Cuadrado
\item Triangulo
\end{itemize}

\subsection{Trabajo Futuro}

Definir las acciones que se realizarán al dibujar cada figura geométrica en el mundo virtual.

Buscar alguna optimización para el algoritmo de detección de patrones para una mejor ejecución. 
\textit{Idea}: Intentar optimizar cada una de las partes del algoritmo, a saber:

\begin{itemize}
\item Algoritmo Douglas Peucker para la simplificación del polígono.
\item Algoritmo Convex Hull.
\item Algoritmo Triángulo de máxima área dentro de un Convex Hull
\item Algoritmo Rectángulo de mínima área que envuelve un Convex Hull
\end{itemize}

%--- Sección de bibliografía ---%
\break

\begin{thebibliography}{X}

%-- ITEM 1 --%
\bibitem[Porcino et al.]{Porcino et al.}
\textsc{Thiago M. Porcino, Esteban Clua, Daniela Trevisan, Cristina N. Vasconcelos and Luis Valente}\\
\textit{Minimizing cyber sickness in head mounted display systems: design guidelines and applications}, Fluminense Federal University, Institute of Computing, 2016

%-- ITEM 2 --%
\bibitem[Davis et al.]{Davis et al.}
\textsc{Simon Davis, Keith Nesbitt and Eugene Nalivaiko}\\
\textit{Comparing the onset of cybersickness using the Oculus Rift and two virtual roller coasters}, Proceedings of the 11th Australasian Conference on Interactive Entertainment (IE 2015), vol. 27, 2015, p. 30.

%-- ITEM 3 --%
\bibitem[Carnegie and Rhee]{Carnegie and Rhee}
\textsc{Kieran Carnegie and Taehyun Rhee}\\
\textit{Reducing Visual Discomfort with HMDs Using Dynamic Depth of Field}, Computer Graphics and Applications, IEEE, vol. 35, no. 5, pp. 34–41, 2015.

%-- ITEM 5 --%
\bibitem[Google Docs]{Google Docs}
\textsc{Pedro Cano y José Carlos}\\
\textit{TFM José Carlos 2017}, Página de Google Docs con referencias, 2017.\\
\url{https://docs.google.com/document/d/1C8u0JthWmuzf5IXWOmzsdw0pZUjdrSbCsHPcxWVF1zo/edit}

%-- ITEM 6 --%
\bibitem[Jerald 2016]{Jerald 2016}
\textsc{Jason Jerald}\\
\textit{The VR Book Human-Centered Design for Virtual Reality}, Editorial ACM Books, 2016.

%-- ITEM 7 --%
\bibitem[Unity Tutorials]{Unity Tutorials}
\textsc{Tutoriales Unity}\\
\textit{User Interfaces for VR}, Tutoriales Unity.\\
\url{https://unity3d.com/es/learn/tutorials/topics/virtual-reality/user-interfaces-vr}

%-- ITEM 8 --%
\bibitem[Vanhatalo]{Vanhatalo}
\textsc{Lauri Vanhatalo}\\
\textit{Online Sketch Recognition: Geometric Shapes}, Thesis submitted for examination for the degree of Master of Science in Technology, Helsinki May 29, 2011.

%-- ITEM 9 --%
\bibitem[Rausch et al.]{Rausch et al.}
\textsc{D. Rausch, I. Assenmacher, T. Kuhlen}\\
\textit{3D Sketch Recognition for Interaction in Virtual Environments}, Virtual Reality Group, RWTH Aachen University, Germany, 2010.

%-- ITEM 10 --%
\bibitem[Clifford et al.]{Clifford et al.}
\textsc{Rory M.S. Clifford, Nikita Mae B. Tuanquin, Robert W. Lindeman}\\
\textit{Jedi ForceExtension: Telekinesis as a Virtual Reality Interaction Metaphor}, HIT Lab NZ, University of Canterbury, New Zealand, 2017.

\end{thebibliography}


\end{document}